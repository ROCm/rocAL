{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c9244e",
   "metadata": {},
   "source": [
    "## Rocal Classification training \n",
    "This example showcases a usecase for rocAL classification training with small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import math\n",
    "import tqdm as tqdm\n",
    "import time \n",
    "from amd.rocal.plugin.pytorch import ROCALClassificationIterator\n",
    "from amd.rocal.pipeline import Pipeline\n",
    "import amd.rocal.fn as fn\n",
    "import amd.rocal.types as types\n",
    "from torch.optim import Optimizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce165e2",
   "metadata": {},
   "source": [
    "## Setting Dataset path \n",
    "Here we are setting the dataset path which will be used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "device = torch.device('cpu')\n",
    "data_dir = './Flower102/split_data/' # Run create_classification_flower_dataset.py before running the notebook for dataset creation\n",
    "train_dir = data_dir + '/train'\n",
    "val_dir = data_dir + '/val'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8acc2",
   "metadata": {},
   "source": [
    "## Defining the Pipeline\n",
    "This is defining a pipeline for a classification task. This pipeline will read images from a directory, decode them, apply augmentations and return (image, label) pairs.This pipeline uses image_random_crop then the output is resized to a dimension of (224,224) followed by normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90041830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(data_path, batch_size, num_classes, one_hot, local_rank, world_size, num_thread, crop, rocal_cpu, fp16):\n",
    "    pipe = Pipeline(batch_size=batch_size, num_threads=num_thread, device_id=local_rank, seed=local_rank+10, rocal_cpu=rocal_cpu,\n",
    "                    tensor_dtype=types.FLOAT16 if fp16 else types.FLOAT, tensor_layout=types.NCHW, prefetch_queue_depth=6)\n",
    "    with pipe:\n",
    "        jpegs, labels = fn.readers.file(file_root=data_path)\n",
    "        rocal_device = 'cpu' if rocal_cpu else 'gpu'\n",
    "        decode = fn.decoders.image_random_crop(jpegs, output_type=types.RGB,\n",
    "                                               file_root=data_path, num_shards=world_size, random_shuffle=True)\n",
    "        res = fn.resize(decode, resize_width=224, resize_height=224, interpolation_type=types.TRIANGULAR_INTERPOLATION)\n",
    "        coin_flip = fn.random.coin_flip(probability=0.5)\n",
    "        cmnp = fn.crop_mirror_normalize(res,\n",
    "                                        output_dtype=types.FLOAT,\n",
    "                                        output_layout=types.NCHW,\n",
    "                                        crop=(224, 224),\n",
    "                                        mirror=coin_flip,\n",
    "                                        mean=[0, 0, 0], std=[1, 1, 1])\n",
    "        if (one_hot):\n",
    "            _ = fn.one_hot(labels, num_classes)\n",
    "        pipe.set_outputs(cmnp)\n",
    "    print('rocal \"{0}\" variant'.format(rocal_device))\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60094a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_pipeline(data_path, batch_size, num_classes, one_hot, local_rank, world_size, num_thread, crop, rocal_cpu, fp16):\n",
    "    pipe = Pipeline(batch_size=batch_size, num_threads=num_thread, device_id=local_rank, seed=local_rank + 10, rocal_cpu=rocal_cpu,\n",
    "                    tensor_dtype=types.FLOAT16 if fp16 else types.FLOAT, tensor_layout=types.NCHW, prefetch_queue_depth=2)\n",
    "    with pipe:\n",
    "        jpegs, labels = fn.readers.file(file_root=data_path)\n",
    "        rocal_device = 'cpu' if rocal_cpu else 'gpu'\n",
    "        decode = fn.decoders.image_random_crop(\n",
    "            jpegs, output_type=types.RGB, file_root=data_path, num_shards=world_size, random_shuffle=False)\n",
    "        res = fn.resize(decode, resize_width=224, resize_height=224, interpolation_type=types.TRIANGULAR_INTERPOLATION)\n",
    "        cmnp = fn.crop_mirror_normalize(res,\n",
    "                                        output_dtype=types.FLOAT16 if fp16 else types.FLOAT,\n",
    "                                        output_layout=types.NCHW,\n",
    "                                        crop=(224, 224),\n",
    "                                        mirror=0,\n",
    "                                        mean=[0, 0, 0],\n",
    "                                        std=[1, 1, 1])\n",
    "        if (one_hot):\n",
    "            _ = fn.one_hot(labels, num_classes)\n",
    "        pipe.set_outputs(cmnp)\n",
    "    print('rocal \"{0}\" variant'.format(rocal_device))\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538094db",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "Here the pipeline is created. In order to use this Pipeline, the pipeline has to be built. This is achieved by calling the build function.\n",
    "Then iterator object is created with ROCALClassificationIterator(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = train_pipeline(data_path=train_dir, batch_size=64, num_classes=1, one_hot=0,\n",
    "                      local_rank=1, world_size=1, num_thread=3, crop=10, rocal_cpu='cpu', fp16=False)\n",
    "pipe.build()\n",
    "trainloader = ROCALClassificationIterator(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = val_pipeline(data_path=val_dir, batch_size=64, num_classes=1, one_hot=0, local_rank=1 , world_size=1 , num_thread=3, crop=10, rocal_cpu='cpu', fp16=False)\n",
    "pipe.build()\n",
    "valloader = ROCALClassificationIterator(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, is_test=False):\n",
    "    global total\n",
    "    global correct\n",
    "    batch_size = target.size(0)\n",
    "    total += batch_size    \n",
    "    _, pred = output.max(dim=1)\n",
    "    if is_test:\n",
    "        preds.extend(pred)\n",
    "    correct += torch.sum(pred == target.data)\n",
    "    return  (correct.float()/total) * 100\n",
    "\n",
    "def reset():\n",
    "    global total, correct\n",
    "    global train_loss, test_loss, best_acc\n",
    "    global trn_losses, trn_accs, val_losses, val_accs\n",
    "    total, correct = 0, 0\n",
    "    train_loss, test_loss, best_acc = 0.0, 0.0, 0.0\n",
    "    trn_losses, trn_accs, val_losses, val_accs = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStats(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.losses =[]\n",
    "        self.precs =[]\n",
    "        self.its = []\n",
    "        \n",
    "    def append(self, loss, prec, it):\n",
    "        self.losses.append(loss)\n",
    "        self.precs.append(prec)\n",
    "        self.its.append(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9795e4",
   "metadata": {},
   "source": [
    "## Saving checkpoints\n",
    "The checkpoints are saved and updated if any new best val_acc is acheived. Then the saved checkpoint is used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, is_best, filename='./checkpoint.pth.tar'):\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, filename = './checkpoint.pth.tar'):\n",
    "    sd = torch.load(filename, map_location=lambda storage, loc: storage)\n",
    "    names = set(model.state_dict().keys())\n",
    "    for n in list(sd.keys()): \n",
    "        if n not in names and n+'_raw' in names:\n",
    "            if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]\n",
    "            del sd[n]\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLR(object):\n",
    "    def __init__(self, optim, bn, base_lr=1e-7, max_lr=100):\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.optim = optim\n",
    "        self.bn = bn - 1\n",
    "        ratio = self.max_lr/self.base_lr\n",
    "        self.mult = ratio ** (1/self.bn)\n",
    "        self.best_loss = 1e9\n",
    "        self.iteration = 0\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def calc_lr(self, loss):\n",
    "        self.iteration +=1\n",
    "        if math.isnan(loss) or loss > 4 * self.best_loss:\n",
    "            return -1\n",
    "        if loss < self.best_loss and self.iteration > 1:\n",
    "            self.best_loss = loss\n",
    "            \n",
    "        mult = self.mult ** self.iteration\n",
    "        lr = self.base_lr * mult\n",
    "        \n",
    "        self.lrs.append(lr)\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        return lr\n",
    "        \n",
    "    def plot(self, start=10, end=-5):\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Losses\")\n",
    "        plt.plot(self.lrs[start:end], self.losses[start:end])\n",
    "        plt.xscale('log')\n",
    "        \n",
    "        \n",
    "    def plot_lr(self):\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.plot(self.lrs)\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd494db3",
   "metadata": {},
   "source": [
    "## Defining Optimizer\n",
    "The optimizer object used in inner loop for fast weight updates. In this example Lookahead optimizer is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=5):\n",
    "        assert(0.0 <= alpha <= 1.0)\n",
    "        assert(k >= 1)\n",
    "        self.optimizer = optimizer\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            group['k_counter'] = 0\n",
    "        self.slow_weights = [[param.clone().detach() for param in group['params']] for group in self.param_groups]\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group, slow_Weight in zip(self.param_groups, self.slow_weights):\n",
    "            group['k_counter'] += 1\n",
    "            if group['k_counter'] == self.k:\n",
    "                for param, weight in zip(group['params'], slow_Weight):\n",
    "                    weight.data.add_(self.alpha, (param.data - weight.data))\n",
    "                    param.data.copy_(weight.data)\n",
    "                group['k_counter'] = 0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_dict = self.optimizer.state_dict()\n",
    "        fast_state = fast_dict['state']\n",
    "        param_groups = fast_dict['param_groups']\n",
    "        slow_state = {(id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "                        for k, v in self.state.items()}\n",
    "        return {\n",
    "            'fast_state': fast_state,\n",
    "            'param_groups': param_groups,\n",
    "            'slow_state': slow_state\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_dict = {\n",
    "            'state': state_dict['fast_state'],\n",
    "            'param_groups': state_dict['param_groups']\n",
    "        }\n",
    "        slow_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups']\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_dict)\n",
    "        self.optimizer.load_state_dict(fast_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5358b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0.0\n",
    "test_loss = 0.0\n",
    "best_acc = 0.0\n",
    "trn_losses = []\n",
    "trn_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "total = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8002987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd95fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(clr, model, optimizer=None):\n",
    "\n",
    "    t = tqdm.tqdm(trainloader, leave=False, total=len(trainloader))\n",
    "    running_loss = 0.\n",
    "    avg_beta = 0.98\n",
    "    model.train()\n",
    "    \n",
    "    for i,data in enumerate(t):\n",
    "        input = data[0]\n",
    "        target = data[1]\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        var_ip, var_tg = Variable(input), Variable(target)\n",
    "        output = model(var_ip)\n",
    "        loss = criterion(output, var_tg)\n",
    "    \n",
    "        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n",
    "        smoothed_loss = running_loss / (1 - avg_beta**(i+1))\n",
    "        t.set_postfix(loss=smoothed_loss)\n",
    "    \n",
    "        lr = clr.calc_lr(smoothed_loss)\n",
    "        if lr == -1 :\n",
    "            break\n",
    "        update_lr(optimizer, lr)   \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    trainloader.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a56d6e",
   "metadata": {},
   "source": [
    "## Defining train and test function \n",
    "To train the model, the data iterator has to be looped over, the inputs are feeded to the network, and optimized .Then the model is tested with batch of images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126dc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=0, model=None, optimizer=None):\n",
    "    model.train()\n",
    "    global best_acc\n",
    "    global trn_accs, trn_losses\n",
    "    is_improving = True\n",
    "    counter = 0\n",
    "    running_loss = 0.\n",
    "    avg_beta = 0.98\n",
    "    \n",
    "    for i, ([input],target) in enumerate(trainloader):\n",
    "        bt_start = time.time()\n",
    "        var_ip, var_tg = Variable(input), Variable(target)\n",
    "                                    \n",
    "        output = model(var_ip)\n",
    "        loss = criterion(output, var_tg)\n",
    "        running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n",
    "        smoothed_loss = running_loss / (1 - avg_beta**(i+1))\n",
    "        trn_losses.append(smoothed_loss)\n",
    "            \n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output.data, target)\n",
    "        trn_accs.append(prec)\n",
    "        train_stats.append(smoothed_loss, prec, time.time()-bt_start)\n",
    "        if prec > best_acc :\n",
    "            best_acc = prec\n",
    "            save_checkpoint(model, True)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    trainloader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb16a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model=None):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        global val_accs, val_losses\n",
    "        running_loss = 0.\n",
    "        avg_beta = 0.98\n",
    "        for i, ([input],target) in enumerate(valloader):\n",
    "            bt_start = time.time()\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            var_ip, var_tg = Variable(input), Variable(target)\n",
    "            output = model(var_ip)\n",
    "            loss = criterion(output, var_tg)\n",
    "            running_loss = avg_beta * running_loss + (1-avg_beta) *loss.item()\n",
    "            smoothed_loss = running_loss / (1 - avg_beta**(i+1))\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output.data, target, is_test=True)\n",
    "            test_stats.append(loss.item(), prec, time.time()-bt_start)\n",
    "            val_losses.append(smoothed_loss)\n",
    "            val_accs.append(prec)\n",
    "        valloader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3334cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model=None, sched=None, optimizer=None):\n",
    "    print(\"Epoch\\tTrn_loss\\tVal_loss\\tTrn_acc\\t\\tVal_acc\")\n",
    "    for j in range(epoch):\n",
    "        train(epoch=j, model=model, optimizer=optimizer)\n",
    "        \n",
    "        test(model)\n",
    "        if sched:\n",
    "            sched.step(j)\n",
    "        print(\"{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\"\n",
    "              .format(j+1, trn_losses[-1], val_losses[-1], trn_accs[-1], val_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=102)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "for param in model.fc.parameters():\n",
    "    param.require_grad = True\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "save_checkpoint(model, True, 'before_start_resnet18.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = Lookahead(optim)\n",
    "\n",
    "clr = CLR(optim, len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, 'before_start_resnet18.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "epoch = 10\n",
    "train_stats = AvgStats()\n",
    "test_stats = AvgStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e32e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c43680",
   "metadata": {},
   "source": [
    "## Define a Loss function and optimizer\n",
    "Here Classification Cross-Entropy loss and SGD with momentum is used as loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06585899",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = Lookahead(optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b70407",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model=model, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Total_time \",end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
